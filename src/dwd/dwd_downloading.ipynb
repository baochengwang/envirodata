{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e10ecb-48f9-495f-92a5-a3517517f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from multiprocessing import cpu_count \n",
    "from multiprocessing.pool import ThreadPool\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# ----------------------------------------------\n",
    "#   Constant \n",
    "# ----------------------------------------------\n",
    "\n",
    "# ABBS = {'wind':'ff',\n",
    "#         'air_temperature':'tu',\n",
    "#         'dew_point':'td',\n",
    "#         'moisture': 'tf', \n",
    "#         'solar':'sd',\n",
    "#         'sun':'sd',\n",
    "#         'precipitation':'rr',\n",
    "#         'extreme_wind':'fx',\n",
    "#         'extreme_temperature':'tx'}\n",
    "\n",
    "# ----------------------------------------------\n",
    "#   Input Data\n",
    "# ----------------------------------------------\n",
    "\n",
    "# variable name\n",
    "# vars = ['air_temperature','wind','solar','extreme_temperature','precipitation','extreme_wind'] \n",
    "var = 'air_temperature' \n",
    "\n",
    "t_interval = '10_minutes'  # ['10_minutes','hourly']\n",
    "\n",
    "state= ['Bayern']  # state is a list, wrapped in [] \n",
    "\n",
    "target_folder ='../../0.raw/dwd'\n",
    "\n",
    "year_start = 2010\n",
    "\n",
    "year_end = 2022\n",
    "\n",
    "# ----------------------------------------------\n",
    "#    Check INPUT \n",
    "# ----------------------------------------------\n",
    "# if state is given as string, --> list\n",
    "if isinstance(state,str):\n",
    "    state = [state]\n",
    "\n",
    "# if the target folder does not exist, then create it.\n",
    "if not Path(target_folder).is_dir():\n",
    "    Path(target_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "#    START FUNCTIONS\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "# function to list all data in the url, with an extension of `ext`\n",
    "def listFD(url, ext=''):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    links = [node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "    return(links)\n",
    "\n",
    "\n",
    "\n",
    "def dwd_meta_reader(var,t_res='10_minutes'):\n",
    "\n",
    "    # parent URL folder where all variables at temporal resolution `t_res` are available\n",
    "    p_list = ('https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/'+ \n",
    "                t_res+'/')\n",
    "    \n",
    "    # available variables downloadable\n",
    "    avs = listFD(p_list)\n",
    "\n",
    "    avs = [av.rstrip('/') for av in avs]\n",
    "\n",
    "    # if variable is not available for the defined temporal resolution, return a message!\n",
    "    if not var in avs:\n",
    "        logging.error(f'{t_res} {var} is NOT AVAILABLE for download!')\n",
    "        return\n",
    "    \n",
    "    # url where metadata is stored\n",
    "    base_url = p_list + var + '/historical/'\n",
    "    \n",
    "    url_meta = listFD(base_url,'txt')\n",
    "\n",
    "    # the metadata file is most often ended with '*_Beschreibung_Stationen.txt'; \n",
    "    # It has not been systematically checked whether this holds TURE.\n",
    "    pattern = re.compile(r'Beschreibung_Stationen.txt')\n",
    "    \n",
    "    url_meta = [url for url in url_meta if pattern.search(url)]\n",
    "\n",
    "    # the complete URL for meta data.\n",
    "    url_meta = ''.join([base_url]+url_meta)\n",
    "\n",
    "    # read header \n",
    "    header = pd.read_csv(url_meta, nrows=1, delimiter=\" \",encoding='latin1')\n",
    "    \n",
    "    # read raw data \n",
    "    data = pd.read_fwf(url_meta, widths=[6, 9, 8, 15, 12, 10, 42, 98], header=None, skiprows=2, encoding='latin1')\n",
    "\n",
    "    # rename columns\n",
    "    station_meta = data.rename(columns=dict(zip(data.columns, header.columns)))\n",
    "    \n",
    "    # convert column types\n",
    "    station_meta = station_meta.astype({'Stations_id': 'int',\n",
    "                                   'von_datum':'str',\n",
    "                                   'bis_datum':'str'})\n",
    "\n",
    "    # convert von_datum and bis_datum to datatime.date format\n",
    "    station_meta[['von_datum','bis_datum']] = station_meta[['von_datum','bis_datum']].apply(pd.to_datetime,format='%Y%m%d')\n",
    "\n",
    "    return(station_meta)\n",
    "\n",
    "\n",
    "\n",
    "# function to download a single file  \n",
    "def download_url(args): \n",
    "    url, fn = args[0], args[1] \n",
    "    \n",
    "    try: \n",
    "        r = requests.get(url) \n",
    "        with open(fn, 'wb') as f:\n",
    "            f.write(r.content) \n",
    "    except Exception as e: \n",
    "        print('Exception in download_url():', e)\n",
    "\n",
    "\n",
    "# function to parallel downloading\n",
    "def download_parallel(args):\n",
    "    cpus = cpu_count() \n",
    "    results = ThreadPool(cpus - 1).imap_unordered(download_url, args) \n",
    "\n",
    "\n",
    "# function to list downloadable zip files for variable `var`\n",
    "def dwd_file_list(var,ids,t_res='10_minutes',y_start=2010,y_end=2022,target_folder='./'):\n",
    "\n",
    "    # parent URL folder where all variables at temporal resolution `t_res` are available\n",
    "    p_list = ('https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/'+ \n",
    "                t_res+'/')\n",
    "    \n",
    "    # available variables downloadable\n",
    "    avs = listFD(p_list)\n",
    "\n",
    "    avs = [av.rstrip('/') for av in avs]\n",
    "\n",
    "    # if variable is not available for the defined temporal resolution, return a message!\n",
    "    if not var in avs:\n",
    "        logging.error(f'{t_res} {var} is NOT AVAILABLE for download!')\n",
    "        return\n",
    "    \n",
    "    # url where metadata is stored\n",
    "    base_url = p_list + var + '/historical/'\n",
    "\n",
    "    # file extension for DWD Climate Data\n",
    "    ext = 'zip'\n",
    "\n",
    "    # list all zip files within the base_url\n",
    "    fns = listFD(base_url, ext)\n",
    "\n",
    "    # Creating a dataframe\n",
    "    df = pd.DataFrame({'url':fns})\n",
    "\n",
    "    # Extracting the third part of the strings in the 'url' column\n",
    "    df[['id', 'von_datum','bis_datum']] = df['url'].str.split('_', expand=True).loc[:, [2,3,4]]\n",
    "\n",
    "    df = df.astype({'id':int})\n",
    "\n",
    "    df[['von_datum', 'bis_datum']] = df[['von_datum', 'bis_datum']].apply(pd.to_datetime, format='%Y%m%d')\n",
    "    \n",
    "    df = df[df['id'].isin(ids)]\n",
    "    \n",
    "    df = df[df.bis_datum >= datetime(y_start,1,1)]\n",
    "    \n",
    "    df = df[df.von_datum <= datetime(y_end,1,1)]\n",
    "\n",
    "    # full_urls are URLs for downloading\n",
    "    df = df.assign(full_url=url+df.url,\n",
    "                  local = target_folder+df.url)\n",
    "       \n",
    "    return(df)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#    END FUNCTIONS\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "facc8664-5bca-447b-9602-9c8505e20526",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = dwd_meta_reader(var)\n",
    "\n",
    "# all IDs in defined federal state\n",
    "station_ids = fm.query(f\"Bundesland in {state}\")['Stations_id'].to_list()\n",
    "\n",
    "# all available zip files and their metadata\n",
    "zip_files = dwd_file_list(var,station_ids,y_start = year_start,y_end = year_end,target_folder='../../0.raw/dwd/')\n",
    "\n",
    "urls = zip(zip_files.full_url,zip_files.local)\n",
    "\n",
    "## TO DOWNLOAD ALL FILES USING   \n",
    "\n",
    "# download_parallel(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "858b4afe-8136-4944-b257-0b4c6ea33280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Code: download the first 3 code \n",
    "\n",
    "dat = zip_files.iloc[0:3,:]\n",
    "\n",
    "urls = zip(dat.full_url,dat.local)\n",
    "\n",
    "download_parallel(urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
